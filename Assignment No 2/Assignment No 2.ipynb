{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c127657",
   "metadata": {},
   "source": [
    "Perform bag-of-words approach (count occurrence, normalized count occurrence), TF-IDF on \n",
    "data. Create embeddings using Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19255c7f",
   "metadata": {},
   "source": [
    "1. Install & Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d29a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\samik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\samik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.2)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: click in c:\\users\\samik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\samik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\samik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\samik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\samik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\samik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\samik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\samik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\samik\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading gensim-4.4.0-cp312-cp312-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.1/24.4 MB 10.7 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 5.2/24.4 MB 12.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.4/24.4 MB 15.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.6/24.4 MB 15.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.0/24.4 MB 15.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.4/24.4 MB 15.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.3/24.4 MB 15.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 15.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 15.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 12.6 MB/s  0:00:01\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk scikit-learn gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72702ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f20ed8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\samik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14bcf7",
   "metadata": {},
   "source": [
    "2. Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b05bcbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"AI is changing the world\",\n",
    "    \"Machine learning is a part of AI\",\n",
    "    \"Deep learning powers modern AI\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a3cef5",
   "metadata": {},
   "source": [
    "3. Bag of Words – Count Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb76bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['ai' 'changing' 'deep' 'is' 'learning' 'machine' 'modern' 'of' 'part'\n",
      " 'powers' 'the' 'world']\n",
      "\n",
      "Bag of Words (Count Occurrence):\n",
      "[[1 1 0 1 0 0 0 0 0 0 1 1]\n",
      " [1 0 0 1 1 1 0 1 1 0 0 0]\n",
      " [1 0 1 0 1 0 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "bow_count = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "print(count_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nBag of Words (Count Occurrence):\")\n",
    "print(bow_count.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487edeb",
   "metadata": {},
   "source": [
    "4. Bag of Words – Normalized Count Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c197a22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag of Words (Normalized Count):\n",
      "[[0.2        0.2        0.         0.2        0.         0.\n",
      "  0.         0.         0.         0.         0.2        0.2       ]\n",
      " [0.16666667 0.         0.         0.16666667 0.16666667 0.16666667\n",
      "  0.         0.16666667 0.16666667 0.         0.         0.        ]\n",
      " [0.2        0.         0.2        0.         0.2        0.\n",
      "  0.2        0.         0.         0.2        0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "bow_array = bow_count.toarray()\n",
    "\n",
    "# Normalize by total words in each document\n",
    "normalized_bow = bow_array / bow_array.sum(axis=1, keepdims=True)\n",
    "\n",
    "print(\"\\nBag of Words (Normalized Count):\")\n",
    "print(normalized_bow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a095550",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency – Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "655e1058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vocabulary:\n",
      "['ai' 'changing' 'deep' 'is' 'learning' 'machine' 'modern' 'of' 'part'\n",
      " 'powers' 'the' 'world']\n",
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.29803159 0.50461134 0.         0.38376993 0.         0.\n",
      "  0.         0.         0.         0.         0.50461134 0.50461134]\n",
      " [0.27824521 0.         0.         0.35829137 0.35829137 0.4711101\n",
      "  0.         0.4711101  0.4711101  0.         0.         0.        ]\n",
      " [0.29803159 0.         0.50461134 0.         0.38376993 0.\n",
      "  0.50461134 0.         0.         0.50461134 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"TF-IDF Vocabulary:\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43fac48",
   "metadata": {},
   "source": [
    "6. Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e94b46fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ai', 'is', 'changing', 'the', 'world'], ['machine', 'learning', 'is', 'a', 'part', 'of', 'ai'], ['deep', 'learning', 'powers', 'modern', 'ai']]\n"
     ]
    }
   ],
   "source": [
    "#Tokenize Sentences\n",
    "tokenized_docs = [nltk.word_tokenize(doc.lower()) for doc in documents]\n",
    "print(tokenized_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a3dc27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Word2Vec Model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_docs,\n",
    "    vector_size=50,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2e1868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec Embedding for 'ai':\n",
      "[-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
      " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
      " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
      " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
      "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
      "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
      "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
      " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
      "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
      "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
      " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
      " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
      "  9.9641159e-03  1.8466286e-02]\n"
     ]
    }
   ],
   "source": [
    "#Get Word Embeddings\n",
    "print(\"\\nWord2Vec Embedding for 'ai':\")\n",
    "print(w2v_model.wv['ai'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe1a5cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words similar to 'learning':\n",
      "[('powers', 0.12486250698566437), ('changing', 0.08061248809099197), ('of', 0.07399576157331467), ('ai', 0.04237300902605057), ('deep', 0.018277151510119438), ('is', 0.011071980930864811), ('machine', 0.0013571369927376509), ('the', -0.1094222441315651), ('a', -0.11910455673933029), ('modern', -0.17424818873405457)]\n"
     ]
    }
   ],
   "source": [
    "#Similar Words\n",
    "print(\"\\nWords similar to 'learning':\")\n",
    "print(w2v_model.wv.most_similar('learning'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
